# A Foundation Model for Non-Verbal Vocalization

This repository contains the code for the paper "**voc2vec: A Foundation Model for Non-Verbal Vocalization**", accepted at ICASSP 2025.

[![voc2vec](https://img.shields.io/badge/voc2vec-HuggingFace-yellow)](https://huggingface.co/alkiskoudounas/voc2vec)
[![voc2vec-LS](https://img.shields.io/badge/voc2vecLS-HuggingFace-blue)](https://huggingface.co/alkiskoudounas/voc2vec-ls-pt)
[![voc2vec-AS](https://img.shields.io/badge/voc2vecAS-HuggingFace-green)](https://huggingface.co/alkiskoudounas/voc2vec-as-pt)

### voc2vec

We propose a novel foundation model, **voc2vec**, specifically designed for non-verbal human data leveraging exclusively open-source non-verbal audio datasets. We employ a collection of 10 datasets covering around 125 hours of non-verbal audio.

Experimental results prove that voc2vec is effective in non-verbal vocalization classification, and it outperforms conventional speech and audio foundation models. Moreover, voc2vec consistently outperforms strong baselines, OpenSmile, and emotion2vec, on six different benchmark datasets. 

voc2vec is the first universal representation model for vocalization tasks.

> [!IMPORTANT]  
> ğŸš¨ The repository is under construction and will be updated with code, models, and instructions soon.
