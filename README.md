# A Foundation Model for Non-Verbal Vocalization

This repository contains the code for the paper "voc2vec: A Foundation Model for Non-Verbal Vocalization". The paper is currently under review and this repository will be updated with the link to the paper once it will be published.

### voc2vec

We propose a novel foundation model, **voc2vec**, specifically designed for non-verbal human data leveraging exclusively open-soruce non-verbal audio datasets. We employ a collection of 10 datasets covering around 125 hours of non-verbal audio.

Experimental results prove that voc2vec is effective in non-verbal vocalization classification, and it outperforms conventional speech and audio foundation models. Moreover, voc2vec consistently outperforms strong baselines, OpenSmile and emotion2vec, on six different benchmark datasets. 

voc2vec is the first universal representation model for vocalization tasks.

> [!IMPORTANT]  
> ğŸš¨ The repository is under construction and will be updated with code, models, and instructions after the review process is completed.
